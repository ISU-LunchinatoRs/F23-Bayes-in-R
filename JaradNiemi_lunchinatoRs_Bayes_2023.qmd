---
title: "Bayes in R"
author: "Jarad Niemi"
format: 
  revealjs: 
    embed-resources: true
execute:
  echo: true
---

## Bayesian Statistcs in R

- Learning Bayes
- Specific models
- General modeling


# Learning Bayes

- What is Bayesian Statistics
- STAT 574
- Books
- LearnBayes
- Binomial analysis

## What is Bayesian Statistics

Bayesian statistics is a coherent framework for learning about the world from
data. 
$$p(\theta|y) \propto p(y|\theta)p(\theta)$$
where

- $p(\theta)$ is your prior (belief)
- $p(y|\theta)$ is the data model
- $p(\theta|y)$ is your posterior (belief)


## STAT 574: Introduction to Bayesian Data Analysis

> Probability models and prior distributions; updating priors through the likelihood function. Computational and simulation-based methods for deriving posterior distributions and for estimating parameters. Basic statistical and hierarchical models. Model adequacy and posterior predictive checks. Markov Chain Monte Carlo methods and introduction to WinBUGS or similar software. Emphasis on applications and examples from the social, biological and physical sciences. 

Prereq: (STAT 301 or STAT 326 or STAT 401 or STAT 587); (STAT 341 or STAT 347 or STAT 447 or STAT 588)

## Books

[My list of textbooks for STAT 544 students](https://www.jarad.me/courses/stat544/textbook.html):

- Bayesian Data Analysis 
- Doing Bayesian Data Analysis (used for STAT 574)
- A First Course in Bayesian Statistical Methods
- Bayesian Computation with R (Jim Albert)


## LearnBayes

```{r binomial-inference}
library("LearnBayes")

# From vignette("BinomialInference")
# Informative prior
# NBA players have three-point percentages between 
beta.par <- beta.select(list(p=0.1, x=0.3), list(p=0.8, x=.4)) 
beta.par # beta parameters

# Bogdan Bogdanovic 2022-23 (from https://www.generatormix.com/current-nba-player-generator)
attempts  <- 360 
successes <- 146

# Prior, Likelihood, and Posterior for Bogdan Bogdanovic
triplot(beta.par, c(successes, attempts - successes))
```

## Binomial model (simpler)

```{r binomial-inference2}
# Posterior using a uniform prior
curve(dbeta(x, 1 + successes, 1 + attempts - successes))

# 95% Credible interval (and confidence interval)
qbeta(c(.025, .975), 1 + successes, 1 + attempts - successes)
```
[Confidence interval for a binomial proportion](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Jeffreys_interval)

## Binomial probability comparison in words

Does [adding oxygen in sealed chambers compared to traditional germination boxes increase S. macrostachya germination](https://acsess.onlinelibrary.wiley.com/doi/abs/10.2135/cropsci2014.11.0783)?
If yes, how much does it increase the probability?



## Binomial probability comparison in math

$$Y_i \stackrel{ind}{\sim} Bin(n_i, \theta_i), \quad i=1,2$$
where $i=1$ is traditional and $i=2$ is added oxygen.
Assume prior $p(\theta_1,\theta_2) \propto 1$. 

Interest centers on 
$$\delta = \theta_2 - \theta_1$$ 
and we want to calculate

- $P(\delta > 0 | y_1,y_2)$,
- $\widehat\delta$, and
- a credible interval for $\delta$.


## Binomial probability comparison in code

```{r}
# data: attempts and successes
n <- 200
y1 <- 26 + 9
y2 <- 41 + 54 

# Monte Carlo simulation
theta2 <- rbeta(1e5, 1 + y2, 1 + n - y2)
theta1 <- rbeta(1e5, 1 + y1, 1 + n - y1)
delta  <- theta2 - theta1

# Posterior probability delta > 0
mean(delta > 0)                             

# Point and interval estimates
round(quantile(delta, c(.025,.5,.975)), 2) * 100 
```





# Specific models

- CRAN Bayesian Task View
- Binomial analysis and comparison
- Regression
- Generalized linear models

## CRAN Bayesian Task View

The 
[CRAN Bayesian Task View](https://cran.r-project.org/web/views/Bayesian.html)
provides a collection of R packages that perform Bayesian analyses. 

Maintained by [Jong Hee Park](https://scholar.google.com/citations?user=AtT80EsAAAAJ&hl=en),
Professor, Depart of Political Science and International Relations, 
Seoul National University.

## Default Bayesian Regression 

Every regression you have ever run is a Bayesian regression. 
The default Bayesian regression model is 
$$Y_i = \beta_0 + \beta_1 X_{i,1} + \cdots + \beta_P X_{i,P}, \quad
\epsilon_i \stackrel{ind}{\sim} N(0,\sigma^2)$$
with prior
$$p(\beta,\sigma^2) \propto 1/\sigma^2.$$ 

You can interpret confidence and prediction intervals from a Bayesian 
perspective, e.g. your updated belief. 

## Default Bayesian Regression - Code

Example code
```{r}
m <- lm(breaks ~ wool + tension, data = warpbreaks)
summary(m)$coefficients
confint(m)
predict(m, newdata = data.frame(wool = "B", tension = "L"), 
        interval = "prediction")
```


## Generalized linear mixed models (GLMMs)

Generalized: accommodates binary, count, and continuous data

- Logistic regression
- Poisson regression
- Linear regression

Mixed: accommodates non-independence

- Random (hierarchical/exchangeable)
- Spatial
- Temporal


## arm

The [arm](https://cran.r-project.org/web/packages/arm/index.html) package
can fit generalized linear (non-mixed) models without MCMC.
Logistic regression:

```{r}
library("arm")

m <- bayesglm(LC == "LungCancer" ~ BK + YR, family = binomial(), 
         data = Sleuth3::case2002)
summary(m)
```

Output is very similar to `glm`. 

## rstanarm

The R package 
[rstanarm](https://cran.r-project.org/web/packages/rstanarm/index.html)
provides functionality for fitting many GLMMs (and other models) using 
Stan. 


## rstanarm - regression

```{r rstanarm-lm, output=FALSE}
library("rstanarm")
m <- stan_lm(breaks ~ wool + tension, 
             prior = R2(0.5), 
             data = warpbreaks)

```

```{r, dependson="rstanarm-lm"}
summary(m)
```


## rstanarm - logistic

```{r rstanarm-logistic, output=FALSE}
m <- stan_glm(LC == "LungCancer" ~ YR + BK, 
             family = binomial(), 
             data = Sleuth3::case2002)
```

```{r, dependson="rstanarm-logistic"}
summary(m)
```


# General modeling



